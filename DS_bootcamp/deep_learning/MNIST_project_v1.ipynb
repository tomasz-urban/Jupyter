{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fd56bf4",
   "metadata": {},
   "source": [
    "Description from `kaggle.com`\n",
    "\n",
    "### MNIST (\"Modified National Institute of Standards and Technology\") \n",
    "\n",
    "Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\n",
    "\n",
    "The goal is to correctly identify digits from a dataset of tens of thousands of handwritten images. \n",
    "\n",
    "Practice Skills:\n",
    "- Computer vision fundamentals including simple neural networks\n",
    "- Classification methods such as SVM and K-nearest neighbors\n",
    "\n",
    "Acknowledgements: \n",
    "More details about the dataset, including algorithms that have been tried on it and their levels of success, can be found at http://yann.lecun.com/exdb/mnist/index.html. The dataset is made available under a Creative Commons Attribution-Share Alike 3.0 license.\n",
    "\n",
    "### Data\n",
    "\n",
    "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "837b49eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c267e1e6",
   "metadata": {},
   "source": [
    "### Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "396b2bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info = True, as_supervised=True)\n",
    "# with_info = True - info about dataset\n",
    "# as_supervised set to True loads the data in 2-tuple structure [input, target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "149ab6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "# We get the number of validation samples using mnist info (method from tensorflow datasets) as follows:\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "# This way is only available for tf datasets \n",
    "# To get the full number (not a float) we can change it to int:\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "# We do the same with the number of test samples\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "# We prepare a function to scale images:\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "    return image, label\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51bf01b",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88ae74fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784 \n",
    "output_size = 10 # 10 digits: 0-9\n",
    "hidden_layer_size = 200\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                            tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19fce99",
   "metadata": {},
   "source": [
    "### Optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "503a8e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5095f88f",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bd8435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a00ca5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "540/540 - 6s - loss: 0.2729 - accuracy: 0.9221 - val_loss: 0.1359 - val_accuracy: 0.9622\n",
      "Epoch 2/8\n",
      "540/540 - 3s - loss: 0.1073 - accuracy: 0.9674 - val_loss: 0.0931 - val_accuracy: 0.9730\n",
      "Epoch 3/8\n",
      "540/540 - 3s - loss: 0.0722 - accuracy: 0.9776 - val_loss: 0.0741 - val_accuracy: 0.9765\n",
      "Epoch 4/8\n",
      "540/540 - 3s - loss: 0.0525 - accuracy: 0.9838 - val_loss: 0.0534 - val_accuracy: 0.9845\n",
      "Epoch 5/8\n",
      "540/540 - 3s - loss: 0.0417 - accuracy: 0.9868 - val_loss: 0.0460 - val_accuracy: 0.9842\n",
      "Epoch 6/8\n",
      "540/540 - 3s - loss: 0.0310 - accuracy: 0.9907 - val_loss: 0.0385 - val_accuracy: 0.9880\n",
      "Epoch 7/8\n",
      "540/540 - 3s - loss: 0.0275 - accuracy: 0.9909 - val_loss: 0.0304 - val_accuracy: 0.9898\n",
      "Epoch 8/8\n",
      "540/540 - 3s - loss: 0.0225 - accuracy: 0.9926 - val_loss: 0.0244 - val_accuracy: 0.9910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x187117a4190>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, epochs = NUM_EPOCHS, validation_data = (validation_inputs, validation_targets), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24965303",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cd416d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 0.0767 - accuracy: 0.9795\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26ea83c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.08. Test accuracy: 97.95\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}'.format(test_loss, test_accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b4ed83",
   "metadata": {},
   "source": [
    "#### After that step we cannot change anything in the model without reseting everything. Now the model \"had seen\" all the data including the test data.\n",
    "#### We got test accuracy very close to the validation accuracy which means we didn't overfit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe1e50a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
