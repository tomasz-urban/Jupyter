{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f4459e6",
   "metadata": {},
   "source": [
    "### Audiobooks project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5493114b",
   "metadata": {},
   "source": [
    "### Problem\n",
    "\n",
    "You are given data from an Audiobook App. Logically, it relates to the audio versions of books ONLY. Each customer in the database has made a purchase at least once, that's why he/she is in the database. We want to create a machine learning algorithm based on our available data that can predict if a customer will buy again from the Audiobook company.\n",
    "\n",
    "The main idea is that if a customer has a low probability of coming back, there is no reason to spend any money on advertising to him/her. If we can focus our efforts SOLELY on customers that are likely to convert again, we can make great savings. Moreover, this model can identify the most important metrics for a customer to come back again. Identifying new customers creates value and growth opportunities.\n",
    "\n",
    "You have a .csv summarizing the data. There are several variables: Customer ID, ), Book length overall (sum of the minute length of all purchases), Book length avg (average length in minutes of all purchases), Price paid_overall (sum of all purchases) ,Price Paid avg (average of all purchases), Review (a Boolean variable whether the customer left a review), Review out of 10 (if the customer left a review, his/her review out of 10, Total minutes listened, Completion (from 0 to 1), Support requests (number of support requests; everything from forgotten password to assistance for using the App), and Last visited minus purchase date (in days).\n",
    "\n",
    "These are the inputs (excluding customer ID, as it is completely arbitrary. It's more like a name, than a number).\n",
    "\n",
    "The targets are a Boolean variable (0 or 1). We are taking a period of 2 years in our inputs, and the next 6 months as targets. So, in fact, we are predicting if: based on the last 2 years of activity and engagement, a customer will convert in the next 6 months. 6 months sounds like a reasonable time. If they don't convert after 6 months, chances are they've gone to a competitor or didn't like the Audiobook way of digesting information. \n",
    "\n",
    "The task is simple: create a machine learning algorithm, which is able to predict if a customer will buy again. \n",
    "\n",
    "This is a classification problem with two classes: won't buy and will buy, represented by 0s and 1s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d616b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a803c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9400e+02, 1.6200e+03, 1.6200e+03, ..., 5.0000e+00, 9.2000e+01,\n",
       "        0.0000e+00],\n",
       "       [1.1430e+03, 2.1600e+03, 2.1600e+03, ..., 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00],\n",
       "       [2.0590e+03, 2.1600e+03, 2.1600e+03, ..., 0.0000e+00, 3.8800e+02,\n",
       "        0.0000e+00],\n",
       "       ...,\n",
       "       [3.1134e+04, 2.1600e+03, 2.1600e+03, ..., 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00],\n",
       "       [3.2832e+04, 1.6200e+03, 1.6200e+03, ..., 0.0000e+00, 9.0000e+01,\n",
       "        0.0000e+00],\n",
       "       [2.5100e+02, 1.6740e+03, 3.3480e+03, ..., 0.0000e+00, 0.0000e+00,\n",
       "        1.0000e+00]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the data\n",
    "raw_csv_data = np.loadtxt('data/Audiobooks_data.csv', delimiter = \",\")\n",
    "raw_csv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d76254d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00994</th>\n",
       "      <th>1620</th>\n",
       "      <th>1620.1</th>\n",
       "      <th>19.73</th>\n",
       "      <th>19.73.1</th>\n",
       "      <th>1</th>\n",
       "      <th>10.00</th>\n",
       "      <th>0.99</th>\n",
       "      <th>1603.80</th>\n",
       "      <th>5</th>\n",
       "      <th>92</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14083.000000</td>\n",
       "      <td>14083.000000</td>\n",
       "      <td>14083.000000</td>\n",
       "      <td>14083.000000</td>\n",
       "      <td>14083.000000</td>\n",
       "      <td>14083.000000</td>\n",
       "      <td>14083.000000</td>\n",
       "      <td>14083.000000</td>\n",
       "      <td>14083.000000</td>\n",
       "      <td>14083.000000</td>\n",
       "      <td>14083.000000</td>\n",
       "      <td>14083.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>16773.611943</td>\n",
       "      <td>1591.279646</td>\n",
       "      <td>1678.612796</td>\n",
       "      <td>7.102894</td>\n",
       "      <td>7.542940</td>\n",
       "      <td>0.160690</td>\n",
       "      <td>8.909717</td>\n",
       "      <td>0.125598</td>\n",
       "      <td>189.788585</td>\n",
       "      <td>0.069871</td>\n",
       "      <td>61.932898</td>\n",
       "      <td>0.158844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9691.239210</td>\n",
       "      <td>504.358512</td>\n",
       "      <td>654.861664</td>\n",
       "      <td>4.930700</td>\n",
       "      <td>5.559378</td>\n",
       "      <td>0.367258</td>\n",
       "      <td>0.643363</td>\n",
       "      <td>0.241104</td>\n",
       "      <td>370.905846</td>\n",
       "      <td>0.470342</td>\n",
       "      <td>88.210402</td>\n",
       "      <td>0.365544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>216.000000</td>\n",
       "      <td>216.000000</td>\n",
       "      <td>3.860000</td>\n",
       "      <td>3.860000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8371.500000</td>\n",
       "      <td>1188.000000</td>\n",
       "      <td>1188.000000</td>\n",
       "      <td>5.330000</td>\n",
       "      <td>5.330000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.910000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>16715.000000</td>\n",
       "      <td>1620.000000</td>\n",
       "      <td>1620.000000</td>\n",
       "      <td>5.950000</td>\n",
       "      <td>6.070000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.910000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>25187.500000</td>\n",
       "      <td>2160.000000</td>\n",
       "      <td>2160.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.910000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>194.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>33683.000000</td>\n",
       "      <td>2160.000000</td>\n",
       "      <td>7020.000000</td>\n",
       "      <td>130.940000</td>\n",
       "      <td>130.940000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2160.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>464.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              00994          1620        1620.1         19.73       19.73.1  \\\n",
       "count  14083.000000  14083.000000  14083.000000  14083.000000  14083.000000   \n",
       "mean   16773.611943   1591.279646   1678.612796      7.102894      7.542940   \n",
       "std     9691.239210    504.358512    654.861664      4.930700      5.559378   \n",
       "min        2.000000    216.000000    216.000000      3.860000      3.860000   \n",
       "25%     8371.500000   1188.000000   1188.000000      5.330000      5.330000   \n",
       "50%    16715.000000   1620.000000   1620.000000      5.950000      6.070000   \n",
       "75%    25187.500000   2160.000000   2160.000000      8.000000      8.000000   \n",
       "max    33683.000000   2160.000000   7020.000000    130.940000    130.940000   \n",
       "\n",
       "                  1         10.00          0.99       1603.80             5  \\\n",
       "count  14083.000000  14083.000000  14083.000000  14083.000000  14083.000000   \n",
       "mean       0.160690      8.909717      0.125598    189.788585      0.069871   \n",
       "std        0.367258      0.643363      0.241104    370.905846      0.470342   \n",
       "min        0.000000      1.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      8.910000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      8.910000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      8.910000      0.130000    194.400000      0.000000   \n",
       "max        1.000000     10.000000      1.000000   2160.000000     30.000000   \n",
       "\n",
       "                 92             0  \n",
       "count  14083.000000  14083.000000  \n",
       "mean      61.932898      0.158844  \n",
       "std       88.210402      0.365544  \n",
       "min        0.000000      0.000000  \n",
       "25%        0.000000      0.000000  \n",
       "50%       11.000000      0.000000  \n",
       "75%      105.000000      0.000000  \n",
       "max      464.000000      1.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_d = pd.read_csv('data/Audiobooks_data.csv')\n",
    "raw_d.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7f2c128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the firs column there is ID (we don't use it) and the last column are targets\n",
    "# We have to split this data:\n",
    "unscaled_inputs_all = raw_csv_data[:,1:-1] #all the rows, columns from 1 to -1 (excluded)\n",
    "targets_all = raw_csv_data[:,-1] #all the rows, only last column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edbce36",
   "metadata": {},
   "source": [
    "### Balancing the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2389b143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2237"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting 1's in the targets\n",
    "num_one_targets  = int(np.sum(targets_all))\n",
    "num_one_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ee5b9e",
   "metadata": {},
   "source": [
    "#### In this course the approach for balancing the dataset is to leave only the same amount of targets with 0 value as we have num_one_targets. The rest is going to be deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23e2b241",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_targets_counter = 0\n",
    "indices_to_remove = []\n",
    "\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i] == 0:\n",
    "        zero_targets_counter +=1\n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            indices_to_remove.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aec2ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)\n",
    "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2c03eb",
   "metadata": {},
   "source": [
    "### Standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e002fe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling using sklearn library (standardizing data along any axis)\n",
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e565fe6c",
   "metadata": {},
   "source": [
    "### Shuffling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92cbea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ebff68",
   "metadata": {},
   "source": [
    "#### The data is shuffled randomly so every time we reuse this notebook the data will be shuffled in different way. There is no random seed in here because this is the very basic and simple approach for EDA. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e50d615",
   "metadata": {},
   "source": [
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5906e549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the sizes of train, test and validations sets\n",
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "train_samples_count = int(0.8*samples_count)\n",
    "validation_samples_count = int(0.1*samples_count)\n",
    "test_samples_count = samples_count-train_samples_count-validation_samples_count\n",
    "\n",
    "# Splitting the data (manually)\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c9c7de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1797.0 3579 0.5020955574182733\n",
      "217.0 447 0.4854586129753914\n",
      "223.0 448 0.49776785714285704\n"
     ]
    }
   ],
   "source": [
    "# We can check if the splitted data is balanced as we intended:\n",
    "print(np.sum(train_targets), train_samples_count, np.sum(train_targets/train_samples_count))\n",
    "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets/validation_samples_count))\n",
    "print(np.sum(test_targets), test_samples_count, np.sum(test_targets/test_samples_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718d66de",
   "metadata": {},
   "source": [
    "#### The second columns is the number of samples which for the first row (training set) should be much bigger than for the last two (val and test). In the last column all values should be around 0.5 (50%) which shows us the split. In this case everything is like it should be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf2abdd",
   "metadata": {},
   "source": [
    "### Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84611584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can save the data in the zipped numpy file format (.npz) for future use\n",
    "np.savez('data/audiobooks_data_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('data/audiobooks_data_validation', inputs=validation_inputs, targets = validation_targets)\n",
    "np.savez('data/audiobooks_data_test', inputs=test_inputs, targets=test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71ca433",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
